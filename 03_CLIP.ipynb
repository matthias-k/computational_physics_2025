{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cc82c4f-dbcc-4d44-b8ce-cac97148fcf4",
   "metadata": {},
   "source": [
    "# CLIP\n",
    "\n",
    "In this exercise, you'll learn how you can use pretrained vision-language models for transfer learning as well as zero-shot classification and retrieval.\n",
    "\n",
    "You'll need to install the python packages `open_clip_torch` and `tqdm`, e.g. with `pip install open_clip_torch tqdm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdea387c-139b-4a5d-947a-79e66fc9a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import open_clip\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aacdf1-0c25-4154-9069-ee4b0e2cca30",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For this exercise, we're going to use a small dataset with about 500 images which are sorted into 6 different categories. You can download the dataset from https://nc.mlcloud.uni-tuebingen.de/index.php/s/KmPbJaZ7gB7Fbp9 (about 130MB) and unzip it in the current directory. This should give you a new sub directory `image_dataset`. Now we load the data, split it into a training and a validation part and look at a few examples from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e6386e-8108-4824-b108-ae42e25dbdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder('image_dataset', transform=ToTensor())\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, validation_dataset = torch.utils.data.random_split(dataset, [len(dataset) - 100, 100], generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d6e06-ae47-4cc7-b200-caf716b18feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rst = np.random.RandomState(43)\n",
    "f, axes = plt.subplots(3, 4, figsize=(16, 10))\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    image, label = validation_dataset[rst.randint(len(validation_dataset))]\n",
    "    ax.imshow(image.numpy().transpose(1, 2, 0))\n",
    "    ax.set_title(f\"label: {label}\")\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b052ff-66a2-447c-860b-f2cf4e49ee65",
   "metadata": {},
   "source": [
    "## Linear Classification\n",
    "\n",
    "First, we want to try to classify the images with a simple linear classifier, which we implement in pytorch and train using crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb5a97f-b818-4744-9cee-f2067182817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    \n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d93a28-8e17-4a70-bea5-6bb7649e71e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # we first downsample by a factor of 10 to make things a bit simpler\n",
    "        self.pool1 = torch.nn.AvgPool2d((10, 10))\n",
    "        self.linear1 = torch.nn.Linear(60 * 80 * 3, 6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(x)\n",
    "        x = x.view(-1, 60 * 80 * 3)\n",
    "        x = self.linear1(x)\n",
    "        return x\n",
    "    \n",
    "model = LinearClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b6005-ce61-43aa-af56-d3e624552ab3",
   "metadata": {},
   "source": [
    "We train the classifier for crossentropy. We additionally employ a learning rate decay schedule: After certain numbers of epochs, the learning rate will be decayed by a factor of 10. You have to decide about optimizer, initial learning rate and decay schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab15b9-fdfb-4a9a-a574-61a937a46772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.TODO(model.parameters(), other_arguments)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1000, 2000])  # choose two decay milestones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858c97b7-33d0-4511-ac44-3f8283d1139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = TODO\n",
    "\n",
    "model.train() # prep model for training\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data, target in train_loader:\n",
    "        #print(data.shape)\n",
    "        #print(target.shape)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    lr_scheduler.step()\n",
    "        \n",
    "    # print training statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a98c2a-7508-40f7-a185-50fb8d1afcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(6))\n",
    "class_total = list(0. for i in range(6))\n",
    "\n",
    "model.eval() # prep model for *evaluation*\n",
    "\n",
    "for data, target in val_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(len(target)):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len(val_loader.sampler)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(6):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcedbc18-810d-4f68-955c-4b904b98c702",
   "metadata": {},
   "source": [
    "#### Exercise 3.1\n",
    "\n",
    "Try to find good choices for optimizer, initial learning rate and decay schedule. Does the linear classifier reach a good performance on the validation set? If not, what kind of reasons might be responsible for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d330b39-2d8c-4e15-9671-19ecc16d04fe",
   "metadata": {},
   "source": [
    "*put your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0f9bcb-cafb-41b6-8a52-0a2872805edb",
   "metadata": {},
   "source": [
    "### Linear Classifier on CLIP embedding\n",
    "\n",
    "In this section, we'll employ transfer learning: Instead of learning a model from scratch, we'll put a linear readout on top of a pretrained CLIP vision encoder. We'll keep the CLIP model fixed and only train the linear readout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d21896-158c-448d-8414-25af3ef9a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and load the CLIP model. Depending on your internet connection, the download will take some time, but it need to be done only once.\n",
    "clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "clip_model.eval()\n",
    "\n",
    "# the tokenizer is a preprocessing for the text encoder that we'll need later.\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841a4857-63e5-4c10-b323-06083421305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the CLIP model, we need a different preprocessing, which conveniently has been provided by `create_model_and_transforms`.\n",
    "# This preprocessing transforms and crops the image to the required size for the model\n",
    "# Because of this, we need to build dataset and data loaders again.\n",
    "\n",
    "dataset = ImageFolder('image_dataset', transform=preprocess)\n",
    "generator = torch.Generator().manual_seed(42)  # set seed to make split deterministic\n",
    "train_dataset, validation_dataset = torch.utils.data.random_split(dataset, [len(dataset) - 100, 100], generator=generator)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    \n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29170f76-295f-4431-b7bc-73a695f84f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = clip_model\n",
    "        # freeze the CLIP parameters\n",
    "        for parameter in self.backbone.parameters():\n",
    "            parameter.requires_grad = False\n",
    "        self.readout = torch.nn.Linear(512, 6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # we don't want to train the CLIP model\n",
    "        with torch.no_grad():\n",
    "            embedding = self.backbone.encode_image(x)\n",
    "        \n",
    "        logits = self.readout(embedding)\n",
    "        return logits\n",
    "    \n",
    "    def train(self, mode=True):\n",
    "        super().train(mode=mode)\n",
    "        # keep backbone in evalm mode\n",
    "        self.backbone.eval()\n",
    "    \n",
    "model = CLIPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da2b52e-d21e-4e82-b876-5ae89f6b6eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "\n",
    "optimizer = torch.optim.TODO(model.parameters(), ...)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1000,2000,3000]) # adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be6115d-8aa5-4eb3-8995-de3a6ff2d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 1000000  # adapt\n",
    "\n",
    "model.train() # prep model for training\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data, target in train_loader:\n",
    "        #print(data.shape)\n",
    "        #print(target.shape)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        #print(data)\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    lr_scheduler.step()\n",
    "        \n",
    "    # print training statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87841b2-a64f-435c-9635-ce33a6f77ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(6))\n",
    "class_total = list(0. for i in range(6))\n",
    "\n",
    "model.eval() # prep model for *evaluation*\n",
    "\n",
    "for data, target in val_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(len(target)):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len(val_loader.sampler)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(6):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f18da4-c3d4-484f-95ef-b3244227bb2f",
   "metadata": {},
   "source": [
    "#### Exercise 3.2\n",
    "\n",
    "Select optimizer, initial learning rate and decay schedule to train the linear readout on top of the CLIP model. What kind of performance do you get? Is the result in some sense surprising? How is performance on validation and training set related to the results of the linear classifier trained in 3.1? How do you explain the differences? What kind of differences did you notice in the training dynamcis compared to the purely linear model above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d9f576-aa19-4a51-946c-dea9c828a7d6",
   "metadata": {},
   "source": [
    "*put your answers here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf1825c-0c2d-4860-bbf5-c56516284b90",
   "metadata": {},
   "source": [
    "## CLIP with zero shot text based classification\n",
    "\n",
    "One of the special properties of CLIP models is that they are not just image models, but a vision language model. This facilitates zero shot classification using text queries. This example demonstrates how you can compute scores (logits) for different text queries given an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748f6c08-fa48-4bc7-8cee-0ca10ff849af",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])\n",
    "image = train_dataset[0][0].unsqueeze(0)\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    image_features = clip_model.encode_image(image)\n",
    "    text_features = clip_model.encode_text(text)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_logits = (100.0 * image_features @ text_features.T)\n",
    "    text_probabilities = text_logits.softmax(dim=-1)\n",
    "    print(text_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc2444-480c-4c89-93ec-77301f7a97d3",
   "metadata": {},
   "source": [
    "Use this example to construct a text based classifier. The `text_logits` from the example are essentially what used to be the model output previously. Since we're now in a *zero-shot setting*, there is no training involved and we only need to the evaluation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4f3a73-a962-43b0-be31-014a6be0b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the text queries that characterize the different classes in the dataset. You'll need to find good queries yourself.\n",
    "queries = [\n",
    "    'query for class label 0',\n",
    "    'query for class label 1',\n",
    "    'query for class label 2',\n",
    "    'query for class label 3',\n",
    "    'query for class label 4',\n",
    "    'query for class label 5',\n",
    "]\n",
    "\n",
    "text = tokenizer(queries)\n",
    "text_features = clip_model.encode_text(text)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(6))\n",
    "class_total = list(0. for i in range(6))\n",
    "\n",
    "model.eval() # prep model for *evaluation*\n",
    "\n",
    "for data, target in val_loader:\n",
    "    # TODO: compute text_logits as in the example in the previous cell\n",
    "    # note that the text features are already computed because they\n",
    "    # don't change from image to image\n",
    "    text_logits = ...\n",
    "    \n",
    "    # calculate the loss\n",
    "    loss = criterion(text_logits, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(len(target)):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len(val_loader.sampler)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(6):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30238242-f4b3-40bd-b4e4-4ed27464e99d",
   "metadata": {},
   "source": [
    "#### Exercise 3.3\n",
    "\n",
    "Complete the above snippet for the text-based classifier and make it work. Try to find good text queries for the six different classes. What kind of performance do you get? How did you find the queries? What are advantages and disadvantages compared to the linear classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e2a56-366c-4339-bc84-ee7285487086",
   "metadata": {},
   "source": [
    "*put your answers here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221e14f-a456-4bf1-a6e1-b16906c83a29",
   "metadata": {},
   "source": [
    "### Text based Image Retrieval\n",
    "\n",
    "Image retrieval is the task of finding the best matching image given a certain query that here will be formulated as text. Because no training is needed anymore, we'll now use the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e4864-4fb3-4261-935c-5e5976343ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = ImageFolder('image_dataset')  # we'll need this later to display the original images\n",
    "dataset = ImageFolder('image_dataset', transform=preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a005ca-5a54-43d1-801e-6afe3716a030",
   "metadata": {},
   "source": [
    "To do so, we first need to build a database of CLIP embeddings for all candidate images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918cfc11-b89a-445b-859b-5b6843e936ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute normalized images features for each image in the training dataset\n",
    "# and concatenate them into a (N_images, N_features) dimensional torch tensor that then can be used for fast image querying.\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "for image, label in tqdm(dataset):\n",
    "    image_tensor = image.unsqueeze(0)\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        image_features = clip_model.encode_image(image_tensor)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    # save image features\n",
    "    embeddings.append(image_features)\n",
    "    labels.append(label)\n",
    "\n",
    "# build \"embedding database\"\n",
    "embeddings = torch.concat(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c195c537-1dda-464c-a343-767ce7c1fcaf",
   "metadata": {},
   "source": [
    "This is the image that we want to find in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7ba7de-be9c-480f-83de-d64c600c9cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(raw_dataset[100][0])\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d559e2-96a2-4da7-8233-4ebfd10924a8",
   "metadata": {},
   "source": [
    "To find images matching a text query, instead of matching one image against multiple texts, we'll now match many images against one text. By sorting the images according to the returned CLIP scores, we can display the most promising matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa7bf4-ab4a-426a-a34f-dec9af0314ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'some image description'  # fill in\n",
    "\n",
    "text = tokenizer(query)\n",
    "\n",
    "# compute probabilities for each image given the text query\n",
    "# this is very similar to the snippets above, but be\n",
    "# careful: now we're not interested in probabilites for each text query\n",
    "# but in probabilities for each image. Hence there will be subtle changes\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    text_features = TODO\n",
    "    image_logits = TODO\n",
    "    image_probabilities = TODO\n",
    "    \n",
    "sorted_indices = np.argsort(image_probabilities.numpy())\n",
    "sorted_indices = sorted_indices[::-1]\n",
    "    \n",
    "f, axes = plt.subplots(1, 5, figsize=(16, 4))\n",
    "for index, ax in zip(sorted_indices, axes):\n",
    "    #print(index)\n",
    "    image = raw_dataset[index][0]\n",
    "    ax.imshow(image)\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(f\"probability: {image_probabilities[index].numpy():.0%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b88b18-ce5f-4f13-98eb-08646df15234",
   "metadata": {},
   "source": [
    "#### Excercise 3.4\n",
    "\n",
    "Complete the code above and find a text query that returns the target image as best matching candidate. How well can you separate it from other images? With other words, how high can you drive the assigned probability for the correct image? Descrie how you found the query that you used in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd166fd9-723f-4533-8baa-2f60784a495b",
   "metadata": {},
   "source": [
    "*put your answers here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a48060f-f973-4276-a429-038b5e1b0b31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
